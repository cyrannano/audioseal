{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of AudioSeal watermark resistance to basic audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement BytesIO (from versions: none)\n",
      "ERROR: No matching distribution found for BytesIO\n"
     ]
    }
   ],
   "source": [
    "!pip install BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.signal\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import typing as tp\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining audio manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_lossy_compression(audio, sample_rate):\n",
    "    with io.BytesIO() as inmemoryfile:\n",
    "        audio_segment = AudioSegment(\n",
    "            audio.tobytes(), \n",
    "            frame_rate=sample_rate,\n",
    "            sample_width=audio.dtype.itemsize, \n",
    "            channels=1\n",
    "        )\n",
    "        audio_segment.export(inmemoryfile, format=\"mp3\")\n",
    "        return np.array(AudioSegment.from_file_using_temporary_files(inmemoryfile)\n",
    "                       .get_array_of_samples())\n",
    "\n",
    "def apply_lossless_compression(audio, sample_rate):\n",
    "    with io.BytesIO() as inmemoryfile:\n",
    "        audio_segment = AudioSegment(\n",
    "            audio.tobytes(), \n",
    "            frame_rate=sample_rate,\n",
    "            sample_width=audio.dtype.itemsize, \n",
    "            channels=1\n",
    "        )\n",
    "        audio_segment.export(inmemoryfile, format=\"flac\")\n",
    "        return np.array(AudioSegment.from_file_using_temporary_files(inmemoryfile)\n",
    "                       .get_array_of_samples())\n",
    "\n",
    "def add_noise(audio, noise_type=\"white\", noise_level=0.01):\n",
    "    if noise_type == \"white\":\n",
    "        noise = np.random.normal(0, noise_level, audio.shape)\n",
    "    elif noise_type == \"gaussian\":\n",
    "        noise = np.random.normal(0, noise_level * np.std(audio), audio.shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported noise type\")\n",
    "    return audio + noise\n",
    "\n",
    "def apply_filter(audio, sample_rate, filter_type=\"lowpass\", cutoff=3000):\n",
    "    nyquist = 0.5 * sample_rate\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    if filter_type == \"lowpass\":\n",
    "        b, a = scipy.signal.butter(5, normal_cutoff, btype='low', analog=False)\n",
    "    elif filter_type == \"highpass\":\n",
    "        b, a = scipy.signal.butter(5, normal_cutoff, btype='high', analog=False)\n",
    "    elif filter_type == \"bandpass\":\n",
    "        low, high = cutoff\n",
    "        b, a = scipy.signal.butter(5, [low / nyquist, high / nyquist], btype='band')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported filter type\")\n",
    "    return scipy.signal.lfilter(b, a, audio)\n",
    "\n",
    "def resample_audio(audio, original_sr, target_sr):\n",
    "    return librosa.resample(audio, original_sr, target_sr)\n",
    "\n",
    "def equalize_audio(audio, sample_rate):\n",
    "    equalized_audio = librosa.effects.equalize(audio, sample_rate)\n",
    "    return equalized_audio\n",
    "\n",
    "def add_reverb(audio, sample_rate, reverberance=50):\n",
    "    reverb_audio = librosa.effects.preemphasis(audio)\n",
    "    return reverb_audio\n",
    "\n",
    "def time_scale_modification(audio, rate):\n",
    "    return librosa.effects.time_stretch(audio, rate)\n",
    "\n",
    "def pitch_shift(audio, sample_rate, n_steps):\n",
    "    return librosa.effects.pitch_shift(audio, sample_rate, n_steps)\n",
    "\n",
    "def dynamic_range_compression(audio):\n",
    "    return librosa.effects.percussive(audio)\n",
    "\n",
    "def clip_audio(audio, threshold=0.8):\n",
    "    return np.clip(audio, -threshold, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Secret message: tensor([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from audioseal import AudioSeal\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "secret_message = torch.randint(0, 2, (1, 16), dtype=torch.int32)\n",
    "secret_message = secret_message.to(device)\n",
    "print(f\"Secret message: {secret_message}\")\n",
    "\n",
    "\n",
    "model = AudioSeal.load_generator(\"audioseal_wm_16bits\")\n",
    "detector = AudioSeal.load_detector(\"audioseal_detector_16bits\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "detector = detector.to(device)\n",
    "\n",
    "def generate_watermark_audio(\n",
    "    tensor: torch.Tensor,\n",
    "    sample_rate: int\n",
    ") -> tp.Optional[torch.Tensor]:\n",
    "    try:\n",
    "        global model, device, secret_message\n",
    "        audios = tensor.unsqueeze(0).to(device)\n",
    "        watermarked_audio = model(audios, sample_rate=sample_rate, message=secret_message.to(device), alpha=1)\n",
    "        return watermarked_audio\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while watermarking audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get the confidence score that an audio tensor was watermarked by Audioseal\n",
    "# provided by the Audioseal team\n",
    "def detect_watermark_audio(\n",
    "    tensor: torch.Tensor,\n",
    "    sample_rate: int,\n",
    "    message_threshold: float = 0.50\n",
    ") -> tp.Optional[float]:\n",
    "    try:\n",
    "        global detector, device\n",
    "        # In our analysis we are not concerned with the hidden/embedded message as of now\n",
    "        result, _ = detector.detect_watermark(tensor, sample_rate=sample_rate, message_threshold=message_threshold)\n",
    "        return float(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while detecting watermark: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and audio samples from CREMA-D dataset\n",
    "def load_crema_d_metadata(metadata_path):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    return df\n",
    "\n",
    "def load_audio_samples(actor_id, base_path):\n",
    "    file_paths = []\n",
    "    # find all audio files starting with the actor_id\n",
    "    for file_path in glob.glob(f\"{base_path}/{actor_id}*.wav\"):\n",
    "        file_paths.append(file_path)\n",
    "    \n",
    "    audio_samples = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        audio, sr = load_audio_sample(file_path)\n",
    "        audio_samples.append((audio, sr, file_path))\n",
    "    \n",
    "    return audio_samples\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def load_audio_sample(\n",
    "    file_path: str\n",
    ") -> tp.Optional[tp.Tuple[torch.Tensor, int]]:\n",
    "    try:\n",
    "        wav, sample_rate = torchaudio.load(file_path)\n",
    "        return wav, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error while loading audio: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(df, emotions, levels, genders, races, ethnicities):\n",
    "    filtered_df = df[\n",
    "        df['Emotion'].isin(emotions) &\n",
    "        df['Emotion_Level'].isin(levels) &\n",
    "        df['Gender'].isin(genders) &\n",
    "        df['Race'].isin(races) &\n",
    "        df['Ethnicity'].isin(ethnicities)\n",
    "    ]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expertiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of manipulations\n",
    "manipulations = [\n",
    "    ('Lossy Compression', apply_lossy_compression),\n",
    "    ('Lossless Compression', apply_lossless_compression),\n",
    "    ('White Noise Addition', lambda audio, sr: add_noise(audio, \"white\")),\n",
    "    ('Gaussian Noise Addition', lambda audio, sr: add_noise(audio, \"gaussian\")),\n",
    "    ('Low-pass Filter', lambda audio, sr: apply_filter(audio, sr, \"lowpass\")),\n",
    "    ('High-pass Filter', lambda audio, sr: apply_filter(audio, sr, \"highpass\")),\n",
    "    ('Band-pass Filter', lambda audio, sr: apply_filter(audio, sr, \"bandpass\")),\n",
    "    ('Downsampling', lambda audio, sr: resample_audio(audio, sr, sr // 2)),\n",
    "    ('Upsampling', lambda audio, sr: resample_audio(audio, sr, sr * 2)),\n",
    "    ('Equalization', equalize_audio),\n",
    "    ('Reverberation', add_reverb),\n",
    "    ('Time-Scale Modification', lambda audio, sr: time_scale_modification(audio, 1.5)),\n",
    "    ('Pitch Shifting', lambda audio, sr: pitch_shift(audio, sr, 2)),\n",
    "    ('Dynamic Range Compression', dynamic_range_compression),\n",
    "    ('Clipping', clip_audio)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment(df, base_path):\n",
    "    results = []\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        audio_sr = load_audio_samples(row['ActorID'], base_path)\n",
    "        for audio, sr, fp in tqdm(audio_sr):\n",
    "            audio = generate_watermark_audio(audio, sr)\n",
    "            # open audio with librosa\n",
    "            original_score = detect_watermark_audio(audio, sr)\n",
    "\n",
    "            audio_np = audio.detach().cpu().numpy().squeeze()\n",
    "            if audio_np.ndim == 1:\n",
    "                audio_np = np.expand_dims(audio_np, axis=0)  # Ensure 2D shape (channels, samples)\n",
    "            audio_path = \"temp_audio.wav\"\n",
    "            sf.write(audio_path, audio_np.T, sr)  # Transpose to match (samples, channels) format\n",
    "            \n",
    "\n",
    "            \n",
    "            # Load the audio with librosa\n",
    "            print(audio_path, sr)\n",
    "            audio_librosa, sr_librosa = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "            for name, manipulation in tqdm(manipulations):\n",
    "                manipulated_audio = manipulation(audio_librosa, sr_librosa)\n",
    "                # convert numpy array to torch tensor\n",
    "                manipulated_audio_tensor = torch.tensor(manipulated_audio).float()\n",
    "                # manipulated_audio_tensor = manipulated_audio_tensor.unsqueeze(0)\n",
    "                detection_score = detect_watermark_audio(manipulated_audio_tensor, sr)\n",
    "                results.append({\n",
    "                    'Emotion': fp.split('/')[-1].split('_')[2],\n",
    "                    'Emotion_Level': fp.split('/')[-1].split('_')[3],\n",
    "                    'Gender': row['Sex'],\n",
    "                    'Race': row['Race'],\n",
    "                    'Ethnicity': row['Ethnicity'],\n",
    "                    'Manipulation': name,\n",
    "                    'Detection Score': detection_score,\n",
    "                    'Original Score': original_score\n",
    "                })\n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Manipulation', y='Detection Score', hue='Emotion', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Watermark Detection Scores for Different Audio Manipulations by Emotion')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Manipulation', y='Detection Score', hue='Emotion_Level', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Watermark Detection Scores for Different Audio Manipulations by Emotion Level')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Manipulation', y='Detection Score', hue='Gender', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Watermark Detection Scores for Different Audio Manipulations by Gender')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Manipulation', y='Detection Score', hue='Race', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Watermark Detection Scores for Different Audio Manipulations by Race')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(x='Manipulation', y='Detection Score', hue='Ethnicity', data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Watermark Detection Scores for Different Audio Manipulations by Ethnicity')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_audio.wav 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      " 40%|████      | 6/15 [00:00<00:00, 36.45it/s]\n",
      "  0%|          | 0/80 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n",
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n",
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n",
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n",
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n",
      "Error while detecting watermark: not enough values to unpack (expected 3, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# filtered_metadata = filter_samples(crema_d_metadata, emotions, levels, genders, races, ethnicities)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m filtered_metadata \u001b[38;5;241m=\u001b[39m crema_d_metadata\n\u001b[1;32m---> 17\u001b[0m experiment_results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_audio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m plot_results(experiment_results)\n",
      "Cell \u001b[1;32mIn[21], line 23\u001b[0m, in \u001b[0;36mperform_experiment\u001b[1;34m(df, base_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m audio_librosa, sr_librosa \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path, sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, manipulation \u001b[38;5;129;01min\u001b[39;00m tqdm(manipulations):\n\u001b[1;32m---> 23\u001b[0m     manipulated_audio \u001b[38;5;241m=\u001b[39m \u001b[43mmanipulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_librosa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr_librosa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# convert numpy array to torch tensor\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     manipulated_audio_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(manipulated_audio)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(audio, sr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the list of manipulations\u001b[39;00m\n\u001b[0;32m      2\u001b[0m manipulations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossy Compression\u001b[39m\u001b[38;5;124m'\u001b[39m, apply_lossy_compression),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLossless Compression\u001b[39m\u001b[38;5;124m'\u001b[39m, apply_lossless_compression),\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhite Noise Addition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: add_noise(audio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaussian Noise Addition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: add_noise(audio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgaussian\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow-pass Filter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: apply_filter(audio, sr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlowpass\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m      8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh-pass Filter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: apply_filter(audio, sr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighpass\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m----> 9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBand-pass Filter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: \u001b[43mapply_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbandpass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownsampling\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: resample_audio(audio, sr, sr \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpsampling\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: resample_audio(audio, sr, sr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEqualization\u001b[39m\u001b[38;5;124m'\u001b[39m, equalize_audio),\n\u001b[0;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReverberation\u001b[39m\u001b[38;5;124m'\u001b[39m, add_reverb),\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime-Scale Modification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: time_scale_modification(audio, \u001b[38;5;241m1.5\u001b[39m)),\n\u001b[0;32m     15\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPitch Shifting\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m audio, sr: pitch_shift(audio, sr, \u001b[38;5;241m2\u001b[39m)),\n\u001b[0;32m     16\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDynamic Range Compression\u001b[39m\u001b[38;5;124m'\u001b[39m, dynamic_range_compression),\n\u001b[0;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClipping\u001b[39m\u001b[38;5;124m'\u001b[39m, clip_audio)\n\u001b[0;32m     18\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[16], line 42\u001b[0m, in \u001b[0;36mapply_filter\u001b[1;34m(audio, sample_rate, filter_type, cutoff)\u001b[0m\n\u001b[0;32m     40\u001b[0m     b, a \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mbutter(\u001b[38;5;241m5\u001b[39m, normal_cutoff, btype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, analog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filter_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbandpass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     low, high \u001b[38;5;241m=\u001b[39m cutoff\n\u001b[0;32m     43\u001b[0m     b, a \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mbutter(\u001b[38;5;241m5\u001b[39m, [low \u001b[38;5;241m/\u001b[39m nyquist, high \u001b[38;5;241m/\u001b[39m nyquist], btype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mband\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "metadata_path = '../../../../crema-d/VideoDemographics.csv'  # Placeholder path\n",
    "base_audio_path = '../../../../crema-d'  # Placeholder path\n",
    "crema_d_metadata = load_crema_d_metadata(metadata_path)\n",
    "\n",
    "# Define filters\n",
    "# emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad']\n",
    "# levels = ['Low', 'Medium', 'High', 'Unspecified']\n",
    "emotions = ['ANG', 'DIS', 'FEA', 'HAP', 'NEU', 'SAD']\n",
    "levels = ['LO', 'MD', 'HI', 'XX']\n",
    "genders = ['Male', 'Female']\n",
    "races = ['White', 'Black', 'Asian', 'Other']\n",
    "ethnicities = ['Hispanic', 'Non-Hispanic']\n",
    "\n",
    "# filtered_metadata = filter_samples(crema_d_metadata, emotions, levels, genders, races, ethnicities)\n",
    "filtered_metadata = crema_d_metadata\n",
    "experiment_results = perform_experiment(filtered_metadata, base_audio_path)\n",
    "plot_results(experiment_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
